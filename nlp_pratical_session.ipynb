{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 style=\"text-align: center;\">NLP for One Health</H1>\n",
    "<h3 style=\"text-align: center;\">From BERT to ChatGPT</h3>\n",
    "\n",
    "The use of artificial intelligence (AI) in One Health has the potential to revolutionize disease detection and outbreak response. Natural Language Processing (NLP) is a subfield of AI that enables computers to understand and analyze large amounts of text data, including social media posts, news articles or medical records. \n",
    "\n",
    "In this practical session, we will explore how NLP can help in the early detection of outbreaks and monitor crisis situations by social mining of newspapers and social media. Specifically, we will start by discussing BERT, a powerful pre-trained language model that can be fine-tuned for specific NLP tasks. We will then move on to ChatGPT, the most popular large language model that can generate human-like responses to natural language inputs. \n",
    "\n",
    "Through the use of these 2 models, we will discuss practical examples of how NLP can be applied in the One Health context, including case studies of outbreak detection and social media monitoring. Participants will have the opportunity to work with BERT and ChatGPT in hands-on exercises to gain practical experience with these powerful tools. By the end of the session, participants will have a deeper understanding of how NLP can be used in the One Health context and be equipped with the skills to apply these techniques in their own work.\n",
    "\n",
    "\n",
    "|   |   |   |   |\n",
    "|---|---|---|---|\n",
    "| <img src=\"https://mood-h2020.eu/wp-content/uploads/2020/10/logo_Mood_texte-dessous_CMJN_vecto-300x136.jpg\" alt=\"mood\"/> | <img src=\"https://www.murdoch.edu.au/ResourcePackages/Murdoch2021/assets/dist/images/logo.svg\" alt=\"murdoch\" /> | <img src=\"https://www.umr-tetis.fr/images/logo-header-tetis.png\" alt=\"tetis\"/> | <img src=\"https://www.inrae.fr/themes/custom/inrae_socle/logo.svg\" alt=\"INRAE\" /> |\n",
    "\n",
    "Speaker: **Rémy DECOUPES** - Research engineer UMR TETIS / INRAE\n",
    "\n",
    "------------------------\n",
    "\n",
    "# 1. BERT\n",
    "\"[Bidirectional Encoder Representations from Transformers - Devlin et al - 2018](https://arxiv.org/abs/1810.04805)\" from Google Research is an open-source pre-trained Language Model. BERT implements the well known \"[Attention is all you need - Vaswani et al - 2017](https://arxiv.org/abs/1706.03762)\"\n",
    "\n",
    "Bert-case was trained on: \n",
    "+ Wikipedia (2.5 Billions of tokens)\n",
    "+ Google books (0.8 Billions of tokens).\n",
    "\n",
    "On two tasks:\n",
    "+ Self-masking\n",
    "+ Next sentence prediction\n",
    "\n",
    "## 1.1 Transformers\n",
    "A python library to easily work with BERT-like models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.conda/lib/python3.10/site-packages (4.27.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.conda/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: torch in ./.conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.1)\n",
      "Requirement already satisfied: wheel in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.1)\n",
      "Requirement already satisfied: lit in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# installation\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load BERT models\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NLP tasks with BERT\n",
    "Let's use transformers' pipeline on common NLP tasks\n",
    "### 1.2.1 Self-masking\n",
    "Predict a token masked inside a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4010227620601654,\n",
       "  'token': 4279,\n",
       "  'token_str': 'communities',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, communities and our environment.'},\n",
       " {'score': 0.09270986169576645,\n",
       "  'token': 4176,\n",
       "  'token_str': 'animals',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, animals and our environment.'},\n",
       " {'score': 0.06399711221456528,\n",
       "  'token': 2945,\n",
       "  'token_str': 'families',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, families and our environment.'},\n",
       " {'score': 0.06004488468170166,\n",
       "  'token': 2740,\n",
       "  'token_str': 'health',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, health and our environment.'},\n",
       " {'score': 0.05965539067983627,\n",
       "  'token': 4044,\n",
       "  'token_str': 'environment',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, environment and our environment.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"One Health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, [MASK] and our environment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Next sentence prediction\n",
    "The aim of this NLP task is to tell if the 2nd sentence could be after the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def next_sentence_prediction(sentence_1, sentence_2):\n",
    "  encoding = tokenizer.encode_plus(sentence_1, sentence_2, return_tensors='pt')\n",
    "  logits = nsp_model(**encoding)[0] \n",
    "  probs = softmax(logits, dim=1)\n",
    "  return probs[0][0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999943971633911\n",
      "7.740520959487185e-05\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"The One Health Initiative is an interdisciplinary movement to create collaborations between animal, human, and environmental health\"\n",
    "sentence_2 = \"The aim is  to better and more rapidly respond to outbreaks and newly emerging zoonoses and diseases.\"\n",
    "\n",
    "score = next_sentence_prediction(sentence_1, sentence_2)\n",
    "print(score)\n",
    "\n",
    "sentence_3 = \"Murdoch University is located in Perth, Western Australia\"\n",
    "\n",
    "score = next_sentence_prediction(sentence_1, sentence_3)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Name Entities Recognition\n",
    "NER is very usefull to extract specific information. In epidemiology surveillance, we want to extract from new articles the pathogen, the host and the location of an outbreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 829/829 [00:00<00:00, 334kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 433M/433M [00:09<00:00, 45.9MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 19.8kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 708kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 499B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 35.0kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-LOC',\n",
       "  'score': 0.9993818,\n",
       "  'index': 8,\n",
       "  'word': 'Do',\n",
       "  'start': 22,\n",
       "  'end': 24},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.8579382,\n",
       "  'index': 9,\n",
       "  'word': '##rdo',\n",
       "  'start': 24,\n",
       "  'end': 27},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.69484633,\n",
       "  'index': 10,\n",
       "  'word': '##gne',\n",
       "  'start': 27,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', model='dslim/bert-base-NER')\n",
    "ner(\"2 swans found dead in Dordogne\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Diving into the model representations\n",
    "### 1.3.1 Inputs: Natural texts to vectors\n",
    "Inputs / Tokenization.\n",
    "\n",
    "BERT has a vocabulary size of ~30 k Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: {'input_ids': tensor([[  101,  1016, 26699,  2179,  2757,  1999,  2079, 20683, 10177,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} \n",
      "\n",
      "Inputs_ids: tensor([[  101,  1016, 26699,  2179,  2757,  1999,  2079, 20683, 10177,   102]]) \n",
      "\n",
      "Inputs word ids: [None, 0, 1, 2, 3, 4, 5, 5, 5, None] \n",
      "\n",
      "Inputs to words: ['2', 'swans', 'found', 'dead', 'in', 'do', '##rdo', '##gne'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"2 swans found dead in Dordogne\"\n",
    "inputs = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "\n",
    "print(f\"Inputs: {inputs} \\n\")\n",
    "print(f\"Inputs_ids: {inputs['input_ids']} \\n\")\n",
    "print(f\"Inputs word ids: {inputs.word_ids()} \\n\")\n",
    "print(f\"Inputs to words: {tokenizer.tokenize(text)} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on two points:\n",
    "\n",
    "+ **Sub-tokenization**: as BERT knows only ~30K Tokens, tokens unknown have to be splitted into subtoken such as **Dordogne** becomes [do, ##rdo, ##gne].\n",
    "+ **Special token**: Two tokens have been added: 101 and 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID = 2079, 20683, 10177: ['do', '##rdo', '##gne'] \\n\n",
      "\n",
      "\n",
      "Shape of inputs: torch.Size([1, 10]) | Number of words: 8 \\n\n",
      "Token ID = 101: [CLS] \\n\n",
      "Token ID = 102: [SEP] \\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token ID = 2079, 20683, 10177: {tokenizer.convert_ids_to_tokens([2079, 20683, 10177])} \\\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of inputs: {inputs['input_ids'].shape} | Number of words: {len(tokenizer.tokenize(text))} \\\\n\")\n",
    "print(f\"Token ID = 101: {tokenizer.convert_ids_to_tokens(101)} \\\\n\")\n",
    "print(f\"Token ID = 102: {tokenizer.convert_ids_to_tokens(102)} \\\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Embedding: \n",
    "Vector representation of semantic informations of texts.\n",
    "\n",
    "Each known token has a vector representation from the pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embedding matrix: Parameter containing:\n",
      "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
      "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
      "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
      "        ...,\n",
      "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
      "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
      "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
      "       requires_grad=True) \n",
      "\n",
      " Matrix shape: torch.Size([30522, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight\n",
    "\n",
    "print(f\"BERT embedding matrix: {embedding_matrix} \\n\\n Matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of Swan (id=26699): tensor([ 0.0468, -0.0246, -0.0598, -0.0030, -0.0016, -0.0671, -0.0588, -0.0971,\n",
      "        -0.0382, -0.0254, -0.0825, -0.0955, -0.1219, -0.0466, -0.0663,  0.0075,\n",
      "        -0.0503, -0.1312,  0.0393, -0.0481, -0.0599, -0.0542, -0.0561, -0.0083,\n",
      "        -0.0670, -0.0430, -0.0506, -0.0610, -0.0210, -0.0383, -0.0438, -0.0586,\n",
      "        -0.0556, -0.0072, -0.0692,  0.0132, -0.0468, -0.0959,  0.0071, -0.0555,\n",
      "        -0.0735, -0.0285, -0.0796, -0.0525,  0.0290, -0.0085,  0.0139, -0.0831,\n",
      "         0.0227, -0.0705, -0.0141, -0.0019, -0.0500, -0.0995, -0.0804, -0.1143,\n",
      "         0.0064, -0.0393, -0.0772, -0.0838, -0.0026, -0.0778,  0.0286,  0.0003,\n",
      "         0.0084, -0.0638, -0.0006, -0.0951, -0.0569, -0.0372,  0.0146,  0.0021,\n",
      "        -0.0527, -0.1213, -0.1014, -0.0221, -0.0598,  0.0375, -0.0032, -0.0867,\n",
      "        -0.0258,  0.0044,  0.0641, -0.0378, -0.0644, -0.0262, -0.0090, -0.1085,\n",
      "        -0.1133, -0.0132, -0.1233,  0.0030, -0.0634, -0.0621, -0.0803,  0.0014,\n",
      "         0.0246, -0.0185, -0.0212, -0.0885,  0.0031,  0.0019, -0.0140, -0.0087,\n",
      "        -0.0525, -0.0899, -0.1086, -0.0205, -0.0773,  0.0117, -0.0231, -0.0140,\n",
      "        -0.0439, -0.0220, -0.0759,  0.0465,  0.0049, -0.0372, -0.0485, -0.0817,\n",
      "         0.0244,  0.0366, -0.1044, -0.0386, -0.0876, -0.0207, -0.0065, -0.1108,\n",
      "        -0.0411, -0.0535, -0.0123, -0.0370,  0.0609, -0.0432, -0.0356,  0.0085,\n",
      "        -0.0404, -0.0151, -0.0329,  0.0470, -0.0013, -0.0973, -0.0405,  0.0169,\n",
      "        -0.0343, -0.1101, -0.0377,  0.0017, -0.0980, -0.0602, -0.0193, -0.0858,\n",
      "        -0.1464,  0.0103,  0.0036, -0.0461, -0.0159, -0.0503, -0.1009, -0.0125,\n",
      "         0.0177, -0.0014, -0.0411, -0.0888,  0.0095, -0.0641, -0.0393, -0.0220,\n",
      "         0.0706, -0.0545, -0.0631, -0.0299, -0.0883,  0.0160, -0.0115,  0.0605,\n",
      "        -0.0738, -0.1061,  0.0063, -0.0878, -0.0636, -0.1145, -0.0783, -0.0146,\n",
      "        -0.0095,  0.0079, -0.0326, -0.0436, -0.0189, -0.0640, -0.0692, -0.0989,\n",
      "        -0.1522, -0.0732, -0.0394, -0.1012, -0.0613, -0.0512, -0.0801, -0.0100,\n",
      "        -0.0453, -0.0241, -0.0290, -0.0688, -0.0391, -0.0040, -0.0251,  0.0276,\n",
      "        -0.0272, -0.0364, -0.0174, -0.0046, -0.0254, -0.0468, -0.0955, -0.0214,\n",
      "        -0.1712,  0.0228,  0.0218, -0.0096, -0.0543,  0.0030, -0.0352, -0.0390,\n",
      "        -0.0836, -0.0660,  0.0136, -0.0986, -0.0032, -0.0247, -0.0677, -0.0191,\n",
      "        -0.0620,  0.0118, -0.0306, -0.0585,  0.0015,  0.0121,  0.0025, -0.0504,\n",
      "        -0.0809,  0.0499, -0.1428, -0.0218, -0.0099,  0.0745, -0.0433, -0.0565,\n",
      "        -0.0550, -0.0096, -0.0604,  0.0041, -0.0099, -0.1038, -0.0226, -0.0179,\n",
      "        -0.0765, -0.0038,  0.0070,  0.0046, -0.1004, -0.0894,  0.0201,  0.0017,\n",
      "        -0.1034, -0.0492,  0.0041, -0.0645, -0.1059, -0.0379, -0.0784, -0.0004,\n",
      "        -0.0318, -0.0041, -0.1115, -0.0140, -0.0753, -0.0250, -0.0281,  0.0056,\n",
      "        -0.0258, -0.0309,  0.0091, -0.0279,  0.0050, -0.0948, -0.0397, -0.0365,\n",
      "        -0.0770, -0.0600, -0.0314,  0.0641, -0.0293, -0.0156, -0.0974, -0.0155,\n",
      "        -0.0061, -0.0573, -0.0197, -0.1060,  0.0060, -0.0268, -0.0018, -0.0092,\n",
      "        -0.0231,  0.0023, -0.0391,  0.0051,  0.0174, -0.0759, -0.0797, -0.0014,\n",
      "        -0.0462, -0.0623, -0.1246, -0.0125, -0.0525, -0.0368, -0.0234,  0.0437,\n",
      "        -0.0066,  0.0340, -0.0258, -0.0184, -0.0757, -0.0313, -0.0975,  0.0139,\n",
      "        -0.0681,  0.0124, -0.0754, -0.0029, -0.0416, -0.0049, -0.0630,  0.0579,\n",
      "        -0.1043, -0.1547, -0.0376,  0.0009, -0.0522, -0.0544, -0.0061, -0.0522,\n",
      "         0.0439,  0.0899, -0.0409, -0.1092, -0.0416, -0.1022, -0.0428,  0.0051,\n",
      "        -0.0299, -0.0407, -0.0518, -0.0561, -0.0826, -0.0549, -0.0063, -0.0148,\n",
      "        -0.0417, -0.0736, -0.0773, -0.0854,  0.0229, -0.0504, -0.0188, -0.0157,\n",
      "        -0.0432, -0.0149,  0.0112, -0.0952, -0.0014, -0.0431, -0.0152, -0.1643,\n",
      "        -0.0370, -0.0296, -0.0511,  0.0073,  0.0265, -0.0577, -0.0023, -0.0290,\n",
      "        -0.0352, -0.0353, -0.0163, -0.0329,  0.0102, -0.0491, -0.0292, -0.0296,\n",
      "        -0.0806, -0.0307, -0.0449, -0.0193,  0.0087, -0.0061, -0.0681, -0.0004,\n",
      "         0.0515, -0.1680, -0.0638, -0.0316, -0.0558,  0.0039, -0.0437, -0.0414,\n",
      "        -0.0542, -0.0343, -0.0860, -0.0101, -0.0748, -0.0713,  0.0456, -0.0928,\n",
      "        -0.0255, -0.1185, -0.0161,  0.0113, -0.0025, -0.0527, -0.0118, -0.1089,\n",
      "        -0.0194, -0.0546, -0.0249, -0.0317, -0.0066, -0.0192,  0.0052, -0.1097,\n",
      "        -0.0685,  0.0701, -0.0463,  0.0332, -0.0118, -0.0534, -0.0179,  0.0056,\n",
      "        -0.0399,  0.0153,  0.0072,  0.0412, -0.0432, -0.0657, -0.0371, -0.0662,\n",
      "        -0.0068,  0.0031,  0.0106, -0.0590, -0.0604, -0.0113, -0.0105, -0.1012,\n",
      "        -0.0220,  0.0283, -0.1031, -0.0137, -0.0473, -0.0122, -0.0962, -0.0359,\n",
      "        -0.0034, -0.0761, -0.0296, -0.0493, -0.1178, -0.0449, -0.0661, -0.1106,\n",
      "        -0.0257, -0.0052, -0.0385, -0.1193, -0.0247, -0.0194, -0.0513, -0.0679,\n",
      "        -0.0071,  0.0063,  0.1054, -0.0151, -0.0293,  0.0083, -0.0220,  0.0033,\n",
      "        -0.0180, -0.1268, -0.0435, -0.0004, -0.0373, -0.0953,  0.0500, -0.0408,\n",
      "        -0.1311, -0.0255, -0.0740, -0.0474,  0.0595, -0.0831, -0.0870, -0.0181,\n",
      "        -0.0269, -0.0766, -0.0288, -0.0694, -0.0612, -0.0792, -0.0548, -0.0747,\n",
      "        -0.0153, -0.1390, -0.0442, -0.0406, -0.0114, -0.0085, -0.0359, -0.0403,\n",
      "         0.0215, -0.0719,  0.0567, -0.0304, -0.0356,  0.0122, -0.0164, -0.0534,\n",
      "        -0.0532,  0.0592, -0.0084, -0.0456, -0.0993, -0.0303, -0.0409, -0.0367,\n",
      "        -0.0336, -0.0561, -0.0776, -0.0870, -0.0109,  0.0107, -0.0119, -0.0679,\n",
      "        -0.0633, -0.0125, -0.0483, -0.0183, -0.0685, -0.0660, -0.0490,  0.0102,\n",
      "        -0.1089,  0.0018, -0.0675, -0.0699,  0.0010, -0.0098, -0.0776, -0.0378,\n",
      "         0.0063, -0.0796, -0.0247,  0.0273, -0.0288, -0.0209, -0.0273, -0.0793,\n",
      "        -0.0536, -0.0018, -0.0202,  0.0020, -0.0404, -0.0297, -0.0136, -0.0513,\n",
      "        -0.0349,  0.0503, -0.0370, -0.0034, -0.0331, -0.0063, -0.0481, -0.0316,\n",
      "        -0.1011, -0.0137, -0.0146, -0.0955, -0.0664, -0.0598, -0.0758, -0.0289,\n",
      "        -0.0612, -0.0319, -0.0318,  0.0234, -0.0742, -0.0469, -0.0265, -0.0442,\n",
      "        -0.0256,  0.0323,  0.0020, -0.0162, -0.0206, -0.0642, -0.0722, -0.1006,\n",
      "        -0.1142, -0.0199, -0.1066, -0.0938, -0.0298, -0.1223, -0.0119, -0.0485,\n",
      "        -0.0234, -0.0303, -0.0552, -0.0223, -0.0296, -0.0152,  0.0377, -0.0140,\n",
      "        -0.0646, -0.0233,  0.0382, -0.0806, -0.0592, -0.0289, -0.0587, -0.0615,\n",
      "        -0.0165, -0.0370, -0.0924, -0.0255, -0.0693,  0.0230, -0.0352, -0.1052,\n",
      "        -0.0329, -0.0372, -0.0569, -0.0472, -0.0442, -0.0444, -0.0266, -0.0447,\n",
      "        -0.0774, -0.0423, -0.0445, -0.0103, -0.0707, -0.0135,  0.0173, -0.0634,\n",
      "        -0.0261,  0.0314, -0.0190, -0.0401, -0.0298, -0.0343, -0.0792, -0.0215,\n",
      "        -0.0080, -0.0196,  0.0314, -0.0167, -0.0676, -0.0620,  0.0463, -0.0113,\n",
      "        -0.0699,  0.0400, -0.0251, -0.0110, -0.0828, -0.0448, -0.0627, -0.0748,\n",
      "        -0.0296, -0.0850, -0.0464, -0.0715, -0.0066, -0.0603, -0.0356, -0.0507,\n",
      "        -0.0370, -0.0530, -0.0785,  0.0435, -0.0270, -0.0494,  0.0124, -0.0594,\n",
      "        -0.0431, -0.0758, -0.0441, -0.0833, -0.0710, -0.0172, -0.0378, -0.0944,\n",
      "        -0.0206, -0.0970, -0.0550, -0.0349,  0.0022,  0.0347, -0.0266,  0.0028,\n",
      "        -0.0266, -0.0715,  0.0305, -0.0591, -0.0672,  0.0300, -0.0670, -0.0035,\n",
      "        -0.0522, -0.0205,  0.0010, -0.0089, -0.0328, -0.0139,  0.0358,  0.0305,\n",
      "        -0.0891, -0.0285, -0.0492,  0.0364, -0.0374, -0.0770, -0.0572,  0.0518,\n",
      "        -0.0374, -0.0029, -0.1623, -0.0413,  0.0585, -0.0736, -0.0206, -0.0864,\n",
      "        -0.0614,  0.0383, -0.1006,  0.0332,  0.0158,  0.0196, -0.1339, -0.0689,\n",
      "        -0.0021, -0.1077, -0.0999, -0.0164, -0.0794, -0.0719, -0.0237, -0.0399,\n",
      "        -0.0302,  0.0037, -0.0278, -0.0499, -0.0649, -0.0395,  0.0229, -0.0469],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "Embeddings of Dordogne (id=2079, 20683, 10177): tensor([[ 0.0066, -0.0533,  0.0057,  ..., -0.0140, -0.0522, -0.0088],\n",
      "        [-0.0508, -0.0170, -0.0469,  ..., -0.0885, -0.0201, -0.0079],\n",
      "        [-0.0153, -0.0091, -0.0587,  ..., -0.0664,  0.0069, -0.0479]],\n",
      "       grad_fn=<IndexBackward0>) \\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embeddings of Swan (id=26699): {embedding_matrix[26699]}\\n\\n\")\n",
    "print(f\"Embeddings of Dordogne (id=2079, 20683, 10177): {embedding_matrix[[2079, 20683, 10177]]} \\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'One Health is an approach calling for the collaborative efforts of multiple a a a a a a a a'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation = pipeline('text-generation', model='bert-base-uncased')\n",
    "text_generation(\"One Health is an approach calling for the collaborative efforts of multiple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
