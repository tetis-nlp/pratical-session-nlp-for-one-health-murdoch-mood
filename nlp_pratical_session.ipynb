{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 style=\"text-align: center;\">NLP for One Health</H1>\n",
    "<h3 style=\"text-align: center;\">From BERT to ChatGPT</h3>\n",
    "\n",
    "The use of artificial intelligence (AI) in One Health has the potential to revolutionize disease detection and outbreak response. Natural Language Processing (NLP) is a subfield of AI that enables computers to understand and analyze large amounts of text data, including social media posts, news articles or medical records. \n",
    "\n",
    "In this practical session, we will explore how NLP can help in the early detection of outbreaks and monitor crisis situations by social mining of newspapers and social media. Specifically, we will start by discussing BERT, a powerful pre-trained language model that can be fine-tuned for specific NLP tasks. We will then move on to ChatGPT, the most popular large language model that can generate human-like responses to natural language inputs. \n",
    "\n",
    "Through the use of these 2 models, we will discuss practical examples of how NLP can be applied in the One Health context, including case studies of outbreak detection and social media monitoring. Participants will have the opportunity to work with BERT and ChatGPT in hands-on exercises to gain practical experience with these powerful tools. By the end of the session, participants will have a deeper understanding of how NLP can be used in the One Health context and be equipped with the skills to apply these techniques in their own work.\n",
    "\n",
    "\n",
    "|   |   |   |   |\n",
    "|---|---|---|---|\n",
    "| <img src=\"https://mood-h2020.eu/wp-content/uploads/2020/10/logo_Mood_texte-dessous_CMJN_vecto-300x136.jpg\" alt=\"mood\"/> | <img src=\"https://www.murdoch.edu.au/ResourcePackages/Murdoch2021/assets/dist/images/logo.svg\" alt=\"murdoch\" /> | <img src=\"https://www.umr-tetis.fr/images/logo-header-tetis.png\" alt=\"tetis\"/> | <img src=\"https://www.inrae.fr/themes/custom/inrae_socle/logo.svg\" alt=\"INRAE\" /> |\n",
    "\n",
    "Speaker: **Rémy DECOUPES** - Research engineer UMR TETIS / INRAE\n",
    "\n",
    "------------------------\n",
    "\n",
    "# 1. BERT\n",
    "\"[Bidirectional Encoder Representations from Transformers - Devlin et al - 2018](https://arxiv.org/abs/1810.04805)\" from Google Research is an open-source pre-trained Language Model. BERT implements the well known \"[Attention is all you need - Vaswani et al - 2017](https://arxiv.org/abs/1706.03762)\"\n",
    "\n",
    "Bert-case was trained on: \n",
    "+ Wikipedia (2.5 Billions of tokens)\n",
    "+ Google books (0.8 Billions of tokens).\n",
    "\n",
    "On two tasks:\n",
    "+ Self-masking\n",
    "+ Next sentence prediction\n",
    "\n",
    "## 1.1 Transformers\n",
    "A python library to easily work with BERT-like models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.conda/lib/python3.10/site-packages (4.27.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.conda/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: torch in ./.conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.1)\n",
      "Requirement already satisfied: wheel in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: lit in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: cmake in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.conda/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.conda/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# installation\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load BERT models\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NLP tasks with BERT\n",
    "Let's use transformers' pipeline on common NLP tasks\n",
    "### 1.2.1 Self-masking\n",
    "Predict a token masked inside a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4010227620601654,\n",
       "  'token': 4279,\n",
       "  'token_str': 'communities',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, communities and our environment.'},\n",
       " {'score': 0.09270986169576645,\n",
       "  'token': 4176,\n",
       "  'token_str': 'animals',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, animals and our environment.'},\n",
       " {'score': 0.06399711221456528,\n",
       "  'token': 2945,\n",
       "  'token_str': 'families',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, families and our environment.'},\n",
       " {'score': 0.06004488468170166,\n",
       "  'token': 2740,\n",
       "  'token_str': 'health',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, health and our environment.'},\n",
       " {'score': 0.05965539067983627,\n",
       "  'token': 4044,\n",
       "  'token_str': 'environment',\n",
       "  'sequence': 'one health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, environment and our environment.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"One Health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, [MASK] and our environment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Next sentence prediction\n",
    "The aim of this NLP task is to tell if the 2nd sentence could be after the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def next_sentence_prediction(sentence_1, sentence_2):\n",
    "  encoding = tokenizer.encode_plus(sentence_1, sentence_2, return_tensors='pt')\n",
    "  logits = nsp_model(**encoding)[0] \n",
    "  probs = softmax(logits, dim=1)\n",
    "  return probs[0][0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999943971633911\n",
      "7.740520959487185e-05\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"The One Health Initiative is an interdisciplinary movement to create collaborations between animal, human, and environmental health\"\n",
    "sentence_2 = \"The aim is  to better and more rapidly respond to outbreaks and newly emerging zoonoses and diseases.\"\n",
    "\n",
    "score = next_sentence_prediction(sentence_1, sentence_2)\n",
    "print(score)\n",
    "\n",
    "sentence_3 = \"Murdoch University is located in Perth, Western Australia\"\n",
    "\n",
    "score = next_sentence_prediction(sentence_1, sentence_3)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Name Entities Recognition\n",
    "NER is very usefull to extract specific information. In epidemiology surveillance, we want to extract from new articles the pathogen, the host and the location of an outbreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 829/829 [00:00<00:00, 334kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 433M/433M [00:09<00:00, 45.9MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 59.0/59.0 [00:00<00:00, 19.8kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 708kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 499B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 35.0kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-LOC',\n",
       "  'score': 0.9993818,\n",
       "  'index': 8,\n",
       "  'word': 'Do',\n",
       "  'start': 22,\n",
       "  'end': 24},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.8579382,\n",
       "  'index': 9,\n",
       "  'word': '##rdo',\n",
       "  'start': 24,\n",
       "  'end': 27},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.69484633,\n",
       "  'index': 10,\n",
       "  'word': '##gne',\n",
       "  'start': 27,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', model='dslim/bert-base-NER')\n",
    "ner(\"2 swans found dead in Dordogne\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Diving into the model representations\n",
    "### 1.3.1 Inputs: Natural texts to vectors\n",
    "Inputs / Tokenization.\n",
    "\n",
    "BERT has a vocabulary size of ~30 k Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: {'input_ids': tensor([[  101,  1016, 26699,  2179,  2757,  1999,  2079, 20683, 10177,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} \n",
      "\n",
      "Inputs_ids: tensor([[  101,  1016, 26699,  2179,  2757,  1999,  2079, 20683, 10177,   102]]) \n",
      "\n",
      "Inputs word ids: [None, 0, 1, 2, 3, 4, 5, 5, 5, None] \n",
      "\n",
      "Inputs to words: ['2', 'swans', 'found', 'dead', 'in', 'do', '##rdo', '##gne'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"2 swans found dead in Dordogne\"\n",
    "inputs = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "\n",
    "print(f\"Inputs: {inputs} \\n\")\n",
    "print(f\"Inputs_ids: {inputs['input_ids']} \\n\")\n",
    "print(f\"Inputs word ids: {inputs.word_ids()} \\n\")\n",
    "print(f\"Inputs to words: {tokenizer.tokenize(text)} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on two points:\n",
    "\n",
    "+ **Sub-tokenization**: as BERT knows only ~30K Tokens, tokens unknown have to be splitted into subtoken such as **Dordogne** becomes [do, ##rdo, ##gne].\n",
    "+ **Special token**: Two tokens have been added: 101 and 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID = 2079, 20683, 10177: ['do', '##rdo', '##gne'] \\n\n",
      "\n",
      "\n",
      "Shape of inputs: torch.Size([1, 10]) | Number of words: 8 \\n\n",
      "Token ID = 101: [CLS] \\n\n",
      "Token ID = 102: [SEP] \\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token ID = 2079, 20683, 10177: {tokenizer.convert_ids_to_tokens([2079, 20683, 10177])} \\\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of inputs: {inputs['input_ids'].shape} | Number of words: {len(tokenizer.tokenize(text))} \\\\n\")\n",
    "print(f\"Token ID = 101: {tokenizer.convert_ids_to_tokens(101)} \\\\n\")\n",
    "print(f\"Token ID = 102: {tokenizer.convert_ids_to_tokens(102)} \\\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Embedding: \n",
    "Vector representation of semantic informations of texts.\n",
    "\n",
    "Each known token has a vector representation from the pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embedding matrix: Parameter containing:\n",
      "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
      "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
      "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
      "        ...,\n",
      "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
      "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
      "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
      "       requires_grad=True) \n",
      "\n",
      " Matrix shape: torch.Size([30522, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight\n",
    "\n",
    "print(f\"BERT embedding matrix: {embedding_matrix} \\n\\n Matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings of Swan (id=26699): tensor([ 0.0468, -0.0246, -0.0598, -0.0030, -0.0016, -0.0671, -0.0588, -0.0971,\n",
      "        -0.0382, -0.0254, -0.0825, -0.0955, -0.1219, -0.0466, -0.0663,  0.0075,\n",
      "        -0.0503, -0.1312,  0.0393, -0.0481, -0.0599, -0.0542, -0.0561, -0.0083,\n",
      "        -0.0670, -0.0430, -0.0506, -0.0610, -0.0210, -0.0383, -0.0438, -0.0586,\n",
      "        -0.0556, -0.0072, -0.0692,  0.0132, -0.0468, -0.0959,  0.0071, -0.0555,\n",
      "        -0.0735, -0.0285, -0.0796, -0.0525,  0.0290, -0.0085,  0.0139, -0.0831,\n",
      "         0.0227, -0.0705, -0.0141, -0.0019, -0.0500, -0.0995, -0.0804, -0.1143,\n",
      "         0.0064, -0.0393, -0.0772, -0.0838, -0.0026, -0.0778,  0.0286,  0.0003,\n",
      "         0.0084, -0.0638, -0.0006, -0.0951, -0.0569, -0.0372,  0.0146,  0.0021,\n",
      "        -0.0527, -0.1213, -0.1014, -0.0221, -0.0598,  0.0375, -0.0032, -0.0867,\n",
      "        -0.0258,  0.0044,  0.0641, -0.0378, -0.0644, -0.0262, -0.0090, -0.1085,\n",
      "        -0.1133, -0.0132, -0.1233,  0.0030, -0.0634, -0.0621, -0.0803,  0.0014,\n",
      "         0.0246, -0.0185, -0.0212, -0.0885,  0.0031,  0.0019, -0.0140, -0.0087,\n",
      "        -0.0525, -0.0899, -0.1086, -0.0205, -0.0773,  0.0117, -0.0231, -0.0140,\n",
      "        -0.0439, -0.0220, -0.0759,  0.0465,  0.0049, -0.0372, -0.0485, -0.0817,\n",
      "         0.0244,  0.0366, -0.1044, -0.0386, -0.0876, -0.0207, -0.0065, -0.1108,\n",
      "        -0.0411, -0.0535, -0.0123, -0.0370,  0.0609, -0.0432, -0.0356,  0.0085,\n",
      "        -0.0404, -0.0151, -0.0329,  0.0470, -0.0013, -0.0973, -0.0405,  0.0169,\n",
      "        -0.0343, -0.1101, -0.0377,  0.0017, -0.0980, -0.0602, -0.0193, -0.0858,\n",
      "        -0.1464,  0.0103,  0.0036, -0.0461, -0.0159, -0.0503, -0.1009, -0.0125,\n",
      "         0.0177, -0.0014, -0.0411, -0.0888,  0.0095, -0.0641, -0.0393, -0.0220,\n",
      "         0.0706, -0.0545, -0.0631, -0.0299, -0.0883,  0.0160, -0.0115,  0.0605,\n",
      "        -0.0738, -0.1061,  0.0063, -0.0878, -0.0636, -0.1145, -0.0783, -0.0146,\n",
      "        -0.0095,  0.0079, -0.0326, -0.0436, -0.0189, -0.0640, -0.0692, -0.0989,\n",
      "        -0.1522, -0.0732, -0.0394, -0.1012, -0.0613, -0.0512, -0.0801, -0.0100,\n",
      "        -0.0453, -0.0241, -0.0290, -0.0688, -0.0391, -0.0040, -0.0251,  0.0276,\n",
      "        -0.0272, -0.0364, -0.0174, -0.0046, -0.0254, -0.0468, -0.0955, -0.0214,\n",
      "        -0.1712,  0.0228,  0.0218, -0.0096, -0.0543,  0.0030, -0.0352, -0.0390,\n",
      "        -0.0836, -0.0660,  0.0136, -0.0986, -0.0032, -0.0247, -0.0677, -0.0191,\n",
      "        -0.0620,  0.0118, -0.0306, -0.0585,  0.0015,  0.0121,  0.0025, -0.0504,\n",
      "        -0.0809,  0.0499, -0.1428, -0.0218, -0.0099,  0.0745, -0.0433, -0.0565,\n",
      "        -0.0550, -0.0096, -0.0604,  0.0041, -0.0099, -0.1038, -0.0226, -0.0179,\n",
      "        -0.0765, -0.0038,  0.0070,  0.0046, -0.1004, -0.0894,  0.0201,  0.0017,\n",
      "        -0.1034, -0.0492,  0.0041, -0.0645, -0.1059, -0.0379, -0.0784, -0.0004,\n",
      "        -0.0318, -0.0041, -0.1115, -0.0140, -0.0753, -0.0250, -0.0281,  0.0056,\n",
      "        -0.0258, -0.0309,  0.0091, -0.0279,  0.0050, -0.0948, -0.0397, -0.0365,\n",
      "        -0.0770, -0.0600, -0.0314,  0.0641, -0.0293, -0.0156, -0.0974, -0.0155,\n",
      "        -0.0061, -0.0573, -0.0197, -0.1060,  0.0060, -0.0268, -0.0018, -0.0092,\n",
      "        -0.0231,  0.0023, -0.0391,  0.0051,  0.0174, -0.0759, -0.0797, -0.0014,\n",
      "        -0.0462, -0.0623, -0.1246, -0.0125, -0.0525, -0.0368, -0.0234,  0.0437,\n",
      "        -0.0066,  0.0340, -0.0258, -0.0184, -0.0757, -0.0313, -0.0975,  0.0139,\n",
      "        -0.0681,  0.0124, -0.0754, -0.0029, -0.0416, -0.0049, -0.0630,  0.0579,\n",
      "        -0.1043, -0.1547, -0.0376,  0.0009, -0.0522, -0.0544, -0.0061, -0.0522,\n",
      "         0.0439,  0.0899, -0.0409, -0.1092, -0.0416, -0.1022, -0.0428,  0.0051,\n",
      "        -0.0299, -0.0407, -0.0518, -0.0561, -0.0826, -0.0549, -0.0063, -0.0148,\n",
      "        -0.0417, -0.0736, -0.0773, -0.0854,  0.0229, -0.0504, -0.0188, -0.0157,\n",
      "        -0.0432, -0.0149,  0.0112, -0.0952, -0.0014, -0.0431, -0.0152, -0.1643,\n",
      "        -0.0370, -0.0296, -0.0511,  0.0073,  0.0265, -0.0577, -0.0023, -0.0290,\n",
      "        -0.0352, -0.0353, -0.0163, -0.0329,  0.0102, -0.0491, -0.0292, -0.0296,\n",
      "        -0.0806, -0.0307, -0.0449, -0.0193,  0.0087, -0.0061, -0.0681, -0.0004,\n",
      "         0.0515, -0.1680, -0.0638, -0.0316, -0.0558,  0.0039, -0.0437, -0.0414,\n",
      "        -0.0542, -0.0343, -0.0860, -0.0101, -0.0748, -0.0713,  0.0456, -0.0928,\n",
      "        -0.0255, -0.1185, -0.0161,  0.0113, -0.0025, -0.0527, -0.0118, -0.1089,\n",
      "        -0.0194, -0.0546, -0.0249, -0.0317, -0.0066, -0.0192,  0.0052, -0.1097,\n",
      "        -0.0685,  0.0701, -0.0463,  0.0332, -0.0118, -0.0534, -0.0179,  0.0056,\n",
      "        -0.0399,  0.0153,  0.0072,  0.0412, -0.0432, -0.0657, -0.0371, -0.0662,\n",
      "        -0.0068,  0.0031,  0.0106, -0.0590, -0.0604, -0.0113, -0.0105, -0.1012,\n",
      "        -0.0220,  0.0283, -0.1031, -0.0137, -0.0473, -0.0122, -0.0962, -0.0359,\n",
      "        -0.0034, -0.0761, -0.0296, -0.0493, -0.1178, -0.0449, -0.0661, -0.1106,\n",
      "        -0.0257, -0.0052, -0.0385, -0.1193, -0.0247, -0.0194, -0.0513, -0.0679,\n",
      "        -0.0071,  0.0063,  0.1054, -0.0151, -0.0293,  0.0083, -0.0220,  0.0033,\n",
      "        -0.0180, -0.1268, -0.0435, -0.0004, -0.0373, -0.0953,  0.0500, -0.0408,\n",
      "        -0.1311, -0.0255, -0.0740, -0.0474,  0.0595, -0.0831, -0.0870, -0.0181,\n",
      "        -0.0269, -0.0766, -0.0288, -0.0694, -0.0612, -0.0792, -0.0548, -0.0747,\n",
      "        -0.0153, -0.1390, -0.0442, -0.0406, -0.0114, -0.0085, -0.0359, -0.0403,\n",
      "         0.0215, -0.0719,  0.0567, -0.0304, -0.0356,  0.0122, -0.0164, -0.0534,\n",
      "        -0.0532,  0.0592, -0.0084, -0.0456, -0.0993, -0.0303, -0.0409, -0.0367,\n",
      "        -0.0336, -0.0561, -0.0776, -0.0870, -0.0109,  0.0107, -0.0119, -0.0679,\n",
      "        -0.0633, -0.0125, -0.0483, -0.0183, -0.0685, -0.0660, -0.0490,  0.0102,\n",
      "        -0.1089,  0.0018, -0.0675, -0.0699,  0.0010, -0.0098, -0.0776, -0.0378,\n",
      "         0.0063, -0.0796, -0.0247,  0.0273, -0.0288, -0.0209, -0.0273, -0.0793,\n",
      "        -0.0536, -0.0018, -0.0202,  0.0020, -0.0404, -0.0297, -0.0136, -0.0513,\n",
      "        -0.0349,  0.0503, -0.0370, -0.0034, -0.0331, -0.0063, -0.0481, -0.0316,\n",
      "        -0.1011, -0.0137, -0.0146, -0.0955, -0.0664, -0.0598, -0.0758, -0.0289,\n",
      "        -0.0612, -0.0319, -0.0318,  0.0234, -0.0742, -0.0469, -0.0265, -0.0442,\n",
      "        -0.0256,  0.0323,  0.0020, -0.0162, -0.0206, -0.0642, -0.0722, -0.1006,\n",
      "        -0.1142, -0.0199, -0.1066, -0.0938, -0.0298, -0.1223, -0.0119, -0.0485,\n",
      "        -0.0234, -0.0303, -0.0552, -0.0223, -0.0296, -0.0152,  0.0377, -0.0140,\n",
      "        -0.0646, -0.0233,  0.0382, -0.0806, -0.0592, -0.0289, -0.0587, -0.0615,\n",
      "        -0.0165, -0.0370, -0.0924, -0.0255, -0.0693,  0.0230, -0.0352, -0.1052,\n",
      "        -0.0329, -0.0372, -0.0569, -0.0472, -0.0442, -0.0444, -0.0266, -0.0447,\n",
      "        -0.0774, -0.0423, -0.0445, -0.0103, -0.0707, -0.0135,  0.0173, -0.0634,\n",
      "        -0.0261,  0.0314, -0.0190, -0.0401, -0.0298, -0.0343, -0.0792, -0.0215,\n",
      "        -0.0080, -0.0196,  0.0314, -0.0167, -0.0676, -0.0620,  0.0463, -0.0113,\n",
      "        -0.0699,  0.0400, -0.0251, -0.0110, -0.0828, -0.0448, -0.0627, -0.0748,\n",
      "        -0.0296, -0.0850, -0.0464, -0.0715, -0.0066, -0.0603, -0.0356, -0.0507,\n",
      "        -0.0370, -0.0530, -0.0785,  0.0435, -0.0270, -0.0494,  0.0124, -0.0594,\n",
      "        -0.0431, -0.0758, -0.0441, -0.0833, -0.0710, -0.0172, -0.0378, -0.0944,\n",
      "        -0.0206, -0.0970, -0.0550, -0.0349,  0.0022,  0.0347, -0.0266,  0.0028,\n",
      "        -0.0266, -0.0715,  0.0305, -0.0591, -0.0672,  0.0300, -0.0670, -0.0035,\n",
      "        -0.0522, -0.0205,  0.0010, -0.0089, -0.0328, -0.0139,  0.0358,  0.0305,\n",
      "        -0.0891, -0.0285, -0.0492,  0.0364, -0.0374, -0.0770, -0.0572,  0.0518,\n",
      "        -0.0374, -0.0029, -0.1623, -0.0413,  0.0585, -0.0736, -0.0206, -0.0864,\n",
      "        -0.0614,  0.0383, -0.1006,  0.0332,  0.0158,  0.0196, -0.1339, -0.0689,\n",
      "        -0.0021, -0.1077, -0.0999, -0.0164, -0.0794, -0.0719, -0.0237, -0.0399,\n",
      "        -0.0302,  0.0037, -0.0278, -0.0499, -0.0649, -0.0395,  0.0229, -0.0469],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "\n",
      "Embeddings of Dordogne (id=2079, 20683, 10177): tensor([[ 0.0066, -0.0533,  0.0057,  ..., -0.0140, -0.0522, -0.0088],\n",
      "        [-0.0508, -0.0170, -0.0469,  ..., -0.0885, -0.0201, -0.0079],\n",
      "        [-0.0153, -0.0091, -0.0587,  ..., -0.0664,  0.0069, -0.0479]],\n",
      "       grad_fn=<IndexBackward0>) \\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embeddings of Swan (id=26699): {embedding_matrix[26699]}\\n\\n\")\n",
    "print(f\"Embeddings of Dordogne (id=2079, 20683, 10177): {embedding_matrix[[2079, 20683, 10177]]} \\\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the strength of BERT is to capture the **contextualized** semantics of a word in its sentence. To explore this embeddings, let's see the last layer representation **last_hidden_states**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 10, 768])\n",
      "\n",
      "\n",
      "Embeddings of Swan inside its sentence (word_id=2): tensor([ 9.0190e-01,  9.0987e-02, -8.5999e-01,  1.2963e-01,  2.7044e-01,\n",
      "        -9.6713e-02, -3.0147e-01, -2.9795e-01, -1.0199e-01, -3.4741e-01,\n",
      "         1.0293e-01, -8.1673e-02, -1.5974e-01,  1.9850e-03, -4.1525e-01,\n",
      "        -1.0670e-01, -1.6164e-01,  1.1252e-01,  4.5947e-02,  1.3988e-01,\n",
      "        -9.7612e-02, -3.4728e-01,  8.7718e-02,  1.1722e-01,  2.4149e-01,\n",
      "        -1.2030e-02, -1.5382e-01,  4.6136e-01, -2.2699e-01, -1.4936e-01,\n",
      "         5.7801e-01,  6.1131e-01,  4.1870e-01,  8.8015e-01, -1.7126e-01,\n",
      "         2.7909e-01, -5.0713e-01, -3.0916e-01,  4.5754e-01, -1.0244e-02,\n",
      "         2.5422e-02,  8.3125e-03,  7.3103e-02, -3.9795e-01,  2.6188e-01,\n",
      "         1.8706e-01,  6.5635e-01, -4.7362e-01,  9.5162e-01, -1.3094e-01,\n",
      "        -1.9189e-01,  7.5922e-01, -4.4014e-01, -6.7426e-02, -3.2155e-01,\n",
      "         5.1992e-02,  1.0292e-01, -3.4766e-01,  3.4522e-01,  2.6228e-01,\n",
      "        -4.2734e-02, -4.3913e-02,  1.4669e-01,  1.7422e-01,  3.0186e-01,\n",
      "         8.0245e-01,  7.8801e-02, -2.6955e-01, -2.2233e-01, -1.4268e-01,\n",
      "         1.2116e+00,  1.3346e-01, -4.3984e-01,  1.5887e-01, -1.4368e-01,\n",
      "        -4.3943e-02,  7.6037e-02,  2.5254e-01,  1.9432e-01,  7.6915e-02,\n",
      "         5.0089e-01,  1.7810e-01,  1.2859e+00,  4.2995e-01, -2.4451e-01,\n",
      "        -3.6700e-01,  1.6821e-01, -1.0145e-01, -5.2561e-01, -1.3171e-01,\n",
      "        -5.1265e-01, -4.3587e-01, -1.0820e-01, -1.6659e-01, -1.6930e-01,\n",
      "         1.0641e-01, -2.5162e-01,  1.5309e-01,  1.1712e+00, -5.9698e-01,\n",
      "        -4.0914e-02,  8.7750e-01,  4.6686e-02,  9.4760e-02, -7.4049e-01,\n",
      "        -1.7668e-01,  2.1469e-01,  1.9382e-01, -4.6680e-03, -3.5613e-01,\n",
      "        -2.9285e-01, -1.4471e-01,  2.1536e-01, -4.5361e-01, -5.3734e-02,\n",
      "         8.5829e-01,  5.2764e-01,  1.3461e-01, -4.7958e-01,  2.9845e-01,\n",
      "        -2.1556e-01,  3.7058e-01,  2.0844e-01,  1.1338e+00, -7.1256e-02,\n",
      "        -3.1819e-01,  8.1221e-01, -1.5056e-02, -1.1202e-01, -9.2844e-02,\n",
      "        -4.8810e-01,  8.2387e-01, -4.5378e-01, -5.9619e-01, -2.3887e-01,\n",
      "         3.8674e-01, -1.1446e-01, -2.6776e-02,  2.4953e-01,  1.2865e-01,\n",
      "         4.7665e-01,  4.8230e-01, -6.3263e-01,  5.2333e-01,  1.4256e-01,\n",
      "         5.0002e-01,  5.4221e-01,  1.3064e-01, -4.7336e-01,  1.0195e-01,\n",
      "        -5.7436e-01, -1.5270e-01, -1.5283e-01, -2.6495e-02,  6.6531e-01,\n",
      "         1.4865e-01, -4.2036e-01, -2.8954e-01, -3.2616e-01, -5.2114e-02,\n",
      "         4.3367e-01, -1.1136e-01, -4.5473e-02, -2.8720e-01, -2.4058e-01,\n",
      "        -3.7843e-02, -5.7771e-01,  6.3144e-01, -6.4574e-01, -3.8957e-01,\n",
      "        -8.5108e-02,  3.6027e-02,  3.8389e-01, -4.2371e-01,  8.3054e-02,\n",
      "        -5.7378e-01, -2.2251e-01, -2.1698e-01,  8.8149e-02, -3.8934e-01,\n",
      "        -3.8921e-01, -2.7114e-01,  3.7488e-01,  3.1657e-01,  7.4860e-01,\n",
      "         8.9153e-01,  1.0228e-01,  3.2309e-01,  5.8094e-01, -8.1020e-03,\n",
      "        -4.1286e-01, -6.3950e-01, -4.0118e-01,  4.9465e-01,  2.3728e-01,\n",
      "        -3.7893e-01,  1.8981e-01, -6.6360e-01, -6.6433e-01, -3.5209e-01,\n",
      "        -7.4276e-01, -3.3914e-01,  5.3811e-02,  5.3587e-02, -7.7396e-02,\n",
      "         1.4102e+00,  3.6052e-01,  2.3600e-01, -5.7758e-01,  6.0717e-01,\n",
      "        -7.5724e-01,  2.8072e-01,  8.9211e-01,  4.2977e-01,  7.1430e-02,\n",
      "         1.2092e-01, -2.6054e-02, -4.9188e-02, -1.4238e-01, -4.1337e-02,\n",
      "         6.0793e-01,  3.6382e-01, -7.1367e-01,  5.5300e-01, -2.9612e-01,\n",
      "         6.0527e-01,  1.6588e-01, -5.2357e-01, -7.4565e-02, -2.5072e-01,\n",
      "        -3.6472e-01,  4.3465e-01, -3.6599e-01, -2.7964e-01, -7.7183e-01,\n",
      "         2.4240e-01, -8.9346e-02, -1.0130e-01,  4.0969e-01,  4.1915e-01,\n",
      "        -2.7024e-01,  5.0557e-01,  4.6587e-01,  5.9520e-01,  2.6988e-01,\n",
      "         1.6953e-01, -2.8278e-01, -9.2994e-02,  6.8303e-02,  1.1824e-01,\n",
      "        -4.0774e-02,  2.8597e-01, -3.7434e-01, -4.4265e-01,  2.8363e-01,\n",
      "         1.3344e-01, -6.9459e-02,  6.9434e-01, -2.5977e-01,  2.0508e-01,\n",
      "         2.1155e-01, -1.6389e-01,  1.1745e+00, -2.2237e-01, -1.0911e-01,\n",
      "        -1.8240e-01, -4.0328e-01, -6.1156e-01,  7.6790e-01, -1.4413e-01,\n",
      "        -6.8367e-01,  3.6822e-01,  5.1230e-01,  9.7670e-02, -1.3873e+00,\n",
      "        -1.9713e-01,  1.4746e-01, -9.7706e-02,  3.5921e-01, -2.8149e-01,\n",
      "         3.7099e-01,  4.9915e-01, -2.9452e-01,  2.3470e-01,  4.9005e-02,\n",
      "        -2.0171e-01,  2.3622e-01,  2.6137e-01,  4.3281e-01, -4.1720e-01,\n",
      "        -2.3079e-01,  1.4282e-01, -1.8051e-01, -3.4163e-01, -6.9396e-01,\n",
      "         1.4273e-01,  4.3106e-01,  2.5151e-01,  7.5881e-01,  3.5827e-01,\n",
      "         2.6007e-01,  4.8421e-01, -5.5937e-01, -2.0447e-01,  1.0053e+00,\n",
      "         5.6715e-01, -3.8270e-01,  4.9932e-01, -5.1850e+00, -3.7018e-01,\n",
      "         5.6428e-01, -4.7088e-01,  6.8029e-01, -1.4399e-01, -4.1802e-01,\n",
      "        -2.9525e-01,  2.2297e-01, -8.3934e-01,  3.7631e-01,  1.2103e-01,\n",
      "        -2.1289e-01,  1.0225e-01, -2.5631e-02, -2.0627e-01,  3.1173e-01,\n",
      "        -2.6407e-01,  2.3511e-01,  4.5362e-01, -7.6331e-01,  2.7404e-01,\n",
      "        -1.0682e-01, -9.6948e-02,  8.5668e-02,  2.9854e-01,  9.0610e-02,\n",
      "         2.4515e-02, -2.7136e-01, -4.2736e-01, -4.3595e-01,  6.0469e-01,\n",
      "         5.7867e-01, -3.7946e-01,  3.4940e-01,  8.5914e-02,  1.9330e-01,\n",
      "         4.6986e-01,  1.6449e-02, -2.4066e-01,  2.0136e-01, -2.1354e-01,\n",
      "        -1.4511e-01,  8.2709e-01,  2.9385e-02,  5.1921e-01, -3.8768e-04,\n",
      "         4.1438e-03,  3.5106e-01, -4.5644e-01,  1.5048e-02, -3.2614e-01,\n",
      "         7.8032e-02,  7.4044e-01,  5.2902e-01, -5.2589e-01,  3.7222e-01,\n",
      "         1.1490e-01,  4.1993e-01, -1.5510e-01,  3.3235e-01,  9.4103e-02,\n",
      "        -3.4793e-01,  4.7599e-01,  5.7042e-02,  6.5732e-02, -7.3951e-02,\n",
      "        -1.5865e+00, -6.6467e-02, -4.7549e-01,  1.5109e-01,  4.2016e-01,\n",
      "         5.5194e-01, -1.1765e+00,  3.7363e-01, -5.9498e-01, -7.5126e-02,\n",
      "         1.9688e-01,  5.3155e-02,  3.6909e-01,  3.4838e-01, -9.9185e-03,\n",
      "         6.7497e-01, -1.0532e-01,  4.7132e-01, -7.7365e-01, -4.7540e-01,\n",
      "         5.7081e-01,  5.6063e-02, -1.9426e-01,  1.9229e-01,  5.0543e-01,\n",
      "         6.7727e-01,  5.2912e-02,  5.7642e-01,  4.2693e-02,  2.2438e-01,\n",
      "        -3.3707e-01,  4.3770e-01,  1.8508e-01, -7.8874e-02, -4.8143e-02,\n",
      "         7.2784e-01,  1.4301e-01, -5.0148e-01, -2.0650e-01, -4.5450e-02,\n",
      "         3.5045e-01, -1.3659e-01,  4.2527e-01,  1.8466e-01, -5.5744e-01,\n",
      "         8.7955e-02, -4.6390e-01, -4.2224e-01,  8.4866e-02,  2.9078e-01,\n",
      "         7.8322e-01, -3.2174e-03,  5.8527e-01,  1.9229e-01,  3.3829e-01,\n",
      "        -3.8615e-01, -2.6928e-01,  1.0692e-01,  3.9733e-01, -1.3798e-02,\n",
      "         3.0662e-01, -6.0504e-01, -2.2274e-01,  1.8388e-01, -1.9203e-01,\n",
      "         1.6384e-01, -6.2512e-02,  4.4634e-01, -1.0989e-01, -7.1032e-01,\n",
      "        -4.5841e-01, -1.1693e+00, -2.0554e-01, -3.6086e-02,  7.3244e-03,\n",
      "         4.4894e-01,  4.5191e-01,  4.7410e-01,  4.8329e-01,  1.1673e-01,\n",
      "        -6.5818e-01,  1.3697e-02,  7.4904e-01, -4.1187e-01, -2.4569e-02,\n",
      "        -1.5141e-01,  3.9579e-01, -3.8761e-01, -4.5488e-01,  8.8070e-01,\n",
      "        -4.6887e-01, -1.2531e-01, -2.6395e-01, -5.5026e-01,  8.8823e-02,\n",
      "        -7.7471e-01, -1.2521e-01,  5.4668e-01,  4.8570e-01,  3.7782e-01,\n",
      "         5.4189e-02,  2.2136e-02,  7.2513e-02, -2.4346e-01, -4.8116e-03,\n",
      "         6.3346e-01,  2.4950e-01, -5.8674e-01,  9.6651e-02, -2.7777e-01,\n",
      "        -2.1086e-02,  1.5230e-01,  1.0020e+00, -6.6549e-02,  3.9085e-01,\n",
      "         1.4962e-01,  3.0871e-01, -4.5620e-01, -3.0956e-01,  4.3240e-01,\n",
      "        -6.3800e-01, -3.0352e-01,  4.4883e-01, -6.1354e-01,  6.2522e-01,\n",
      "         1.0490e+00, -8.0860e-03, -4.9698e-01, -3.7314e-01,  2.2246e-01,\n",
      "        -5.4356e-01,  1.4591e-01, -5.5750e-02, -6.7603e-01, -4.4999e-01,\n",
      "        -2.9802e-02,  3.5585e-02, -2.5787e-01, -6.1615e-01,  2.9938e-01,\n",
      "         3.1070e-01, -5.1238e-01,  1.3899e-02,  5.8405e-02, -2.2659e-01,\n",
      "         5.6849e-03, -2.0727e-01,  1.2501e-01,  4.2531e-01, -1.4301e-01,\n",
      "         1.6204e-01, -4.5997e-01,  4.8653e-01, -5.9605e-01, -4.4133e-02,\n",
      "         8.3253e-02, -4.0672e-01, -1.1597e+00, -4.8267e-02, -1.7807e-01,\n",
      "         2.9224e-01, -7.6745e-01, -4.0741e-01, -1.0645e-01, -4.5238e-01,\n",
      "         3.9107e-01,  2.2884e-01,  2.4142e-01, -2.8209e-01, -2.8280e-01,\n",
      "        -4.4551e-01,  4.8609e-01,  1.9986e-01,  3.4382e-01,  9.0963e-01,\n",
      "        -2.3596e-01,  1.3679e-01, -6.8802e-01, -8.2186e-01, -6.8872e-01,\n",
      "        -1.8722e-01, -5.5185e-02, -2.4394e-01,  1.9972e-01,  1.8603e-01,\n",
      "         5.3154e-01, -5.5643e-01, -3.8561e-01,  8.6706e-01, -7.2185e-02,\n",
      "        -1.5735e-01,  4.1176e-01, -7.5790e-01,  2.6226e-01,  4.5528e-01,\n",
      "        -2.5165e-01,  1.0981e+00, -1.5550e-01,  3.6662e-01,  7.3983e-02,\n",
      "         2.0895e-01, -4.9141e-02,  8.1036e-02, -5.4282e-01,  3.8581e-01,\n",
      "        -9.1497e-02, -3.9914e-01,  4.8723e-01,  3.9215e-01, -5.1322e-01,\n",
      "        -2.1785e-01,  1.3884e-01, -2.9571e-01,  1.4712e-01,  9.2894e-01,\n",
      "        -5.0511e-01, -6.9371e-01, -3.7521e-01, -4.4331e-01, -2.9459e-01,\n",
      "         7.1687e-01,  2.0552e-01,  3.4912e-01,  1.6186e-02,  2.2718e-01,\n",
      "        -2.2731e-01,  6.0187e-01,  6.0541e-02,  2.7678e-01, -2.7291e-01,\n",
      "        -8.0418e-03, -5.0537e-01,  5.5913e-01, -5.9971e-01, -3.6312e-02,\n",
      "        -8.6131e-01, -2.8235e-02, -2.3774e-01,  1.2959e-01, -3.4560e-02,\n",
      "        -9.3584e-01, -9.5129e-01, -6.3060e-02,  1.2545e-02,  9.1913e-02,\n",
      "         2.0406e-01, -2.4984e-01,  6.1615e-01, -2.6956e-01, -8.0215e-01,\n",
      "         7.5356e-02,  6.4228e-01, -2.4046e-01, -7.3004e-01, -7.3466e-01,\n",
      "         3.9186e-01, -2.3710e-01, -5.7708e-01, -5.9518e-02, -1.2042e-01,\n",
      "         6.6571e-03, -3.8158e-01, -6.0912e-02, -1.7378e-01, -1.5670e-01,\n",
      "        -2.1344e-01, -3.1959e-01,  2.3811e-01, -4.8965e-01,  2.4952e-01,\n",
      "        -7.4347e-01, -2.3520e-01,  8.0542e-02,  9.5053e-01,  5.3978e-01,\n",
      "         7.9435e-02,  6.4340e-01, -4.3231e-01, -4.0911e-01, -3.2656e-01,\n",
      "        -2.8396e-01,  7.2276e-01,  3.9260e-01,  6.3374e-01, -2.9210e-01,\n",
      "         1.0486e+00, -8.5374e-01, -2.0220e-01,  8.5105e-01, -4.9768e-02,\n",
      "        -1.3548e-01,  7.9800e-01, -4.3266e-01, -1.7677e-01,  6.3943e-02,\n",
      "        -4.9452e-01, -5.2372e-01, -2.2953e-01,  6.9673e-01, -6.3186e-01,\n",
      "        -1.8612e-01, -1.9187e-02, -1.1476e+00,  2.8692e-01, -7.7884e-01,\n",
      "        -8.1983e-01, -2.0486e-01,  5.6294e-01, -5.1752e-02, -1.1469e-01,\n",
      "        -1.1617e-01,  3.8968e-02, -3.2090e-01, -1.3816e-02,  3.7505e-02,\n",
      "        -3.4024e-01,  1.2706e-02,  1.3677e-01, -3.9811e-01, -4.2714e-01,\n",
      "        -5.4298e-01,  8.7665e-02, -5.8883e-01, -2.1583e-01,  4.1753e-01,\n",
      "         2.4317e-01,  2.7835e-01,  7.4409e-02, -8.4974e-01,  3.2739e-01,\n",
      "         1.9011e-01, -8.5526e-01, -1.7878e-01,  5.6392e-01,  2.0216e-01,\n",
      "         2.0276e-01,  1.8608e-01, -1.8884e-01,  9.0968e-02, -5.3422e-01,\n",
      "        -1.5695e-01, -2.3280e-01, -2.1069e-01, -1.3704e-01,  5.5159e-01,\n",
      "        -6.1314e-01,  6.9580e-02,  3.7380e-02, -1.6667e-01,  2.1210e-01,\n",
      "         6.4020e-02,  1.7995e-01, -1.7343e-01, -2.4021e-01,  2.2808e-01,\n",
      "         5.5938e-01,  5.6689e-01,  2.2775e-01, -8.7898e-02,  4.4149e-01,\n",
      "         2.6999e-01, -4.3798e-01,  2.6875e-01, -1.6737e+00,  1.0090e+00,\n",
      "         4.1982e-01, -3.2075e-01, -1.2522e-01, -7.0979e-02, -1.8913e-01,\n",
      "         2.6444e-01,  3.1731e-01,  4.9985e-01,  4.9811e-01,  3.8598e-01,\n",
      "        -6.5141e-01, -5.2013e-01, -2.0860e+00, -3.0045e-01,  1.8599e-01,\n",
      "        -1.7942e-01, -3.4935e-01, -4.1217e-01, -1.4904e-01, -3.7324e-01,\n",
      "        -5.4310e-01,  3.1120e-01,  1.3410e-01, -3.6523e-01, -1.7530e-01,\n",
      "        -7.4658e-01,  4.1601e-02, -3.2521e-01])\n",
      "\n",
      "\n",
      "Euclidean distance between Sawn (in pre-trained) and Swan in our sentence: 12.949665339207069\n",
      "Euclidean distance between dead (in pre-trained) and dead in our sentence: 12.152452361094523\n",
      "Euclidean distance between dead and Swan in our sentence: 14.458841774801673\n",
      "Euclidean distance between dead and Swan in pre-trained: 1.6326528954870763\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import math \n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"2 swans found dead in Dordogne\"\n",
    "input_ids = tokenizer.encode(input_text)\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "last_hidden_states_mean = last_hidden_states.mean(1)\n",
    "\n",
    "print(f\"Embedding shape: {last_hidden_states.shape}\\n\\n\")\n",
    "print(f\"Embeddings of Swan inside its sentence (word_id=2): {last_hidden_states[0, 2, :]}\\n\\n\")\n",
    "euclidean_distance = math.sqrt(sum((embedding_matrix[26699] - last_hidden_states[0, 2, :])**2))\n",
    "print(f\"Euclidean distance between Sawn (in pre-trained) and Swan in our sentence: {euclidean_distance}\")\n",
    "euclidean_distance = math.sqrt(sum((embedding_matrix[2757] - last_hidden_states[0, 4, :])**2))\n",
    "print(f\"Euclidean distance between dead (in pre-trained) and dead in our sentence: {euclidean_distance}\")\n",
    "euclidean_distance = math.sqrt(sum((last_hidden_states[0, 4, :] - last_hidden_states[0, 2, :])**2))\n",
    "print(f\"Euclidean distance between dead and Swan in our sentence: {euclidean_distance}\")\n",
    "euclidean_distance = math.sqrt(sum((embedding_matrix[2757] - embedding_matrix[26699])**2))\n",
    "print(f\"Euclidean distance between dead and Swan in pre-trained: {euclidean_distance}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Visualize sentences embedding\n",
    "The meaning of a sentence is included in this embeddings of its tokens. We propose here too visualize it:\n",
    "\n",
    "+ Retrieve embedding of a sentence (and not at a token-level) using [sentence embedding](https://github.com/UKPLab/sentence-transformers). We are going to use **all_mpnet-base-v2** first-ranked model by [Sentence BERT benchmark](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models).\n",
    "+ Reduce dimension to plot embeddings on a 2-D figure using [UMAP](https://umap-learn.readthedocs.io/en/latest/)\n",
    "+ Visualize it interactively with [plotly.express](https://plotly.com/python/plotly-express/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence-transformers in ./.conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: torchvision in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: nltk in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.conda/lib/python3.10/site-packages (from sentence-transformers) (4.27.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.7)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (67.6.1)\n",
      "Requirement already satisfied: wheel in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.26.1)\n",
      "Requirement already satisfied: lit in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.3.23)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: click in ./.conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./.conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting umap-learn\n",
      "  Using cached umap-learn-0.5.3.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from umap-learn) (1.24.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in ./.conda/lib/python3.10/site-packages (from umap-learn) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.0 in ./.conda/lib/python3.10/site-packages (from umap-learn) (1.10.1)\n",
      "Collecting numba>=0.49\n",
      "  Using cached numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.8.tar.gz (1.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./.conda/lib/python3.10/site-packages (from umap-learn) (4.65.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Using cached llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.10/site-packages (from numba>=0.49->umap-learn) (67.6.1)\n",
      "Requirement already satisfied: joblib>=0.11 in ./.conda/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Building wheels for collected packages: umap-learn, pynndescent\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=eb5ea24239ea2ae55894adafb68b193187f521325b1007b6260dca55d66cd23c\n",
      "  Stored in directory: /home/rdecoupe/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55496 sha256=a1d8fff7ad96e7323c0cdad56f1807b33ba1e6af305f89ffa8f15b97b8cd1a5d\n",
      "  Stored in directory: /home/rdecoupe/.cache/pip/wheels/f8/5c/b4/a06301605095861524c1c7268a0d445b3a4c50292ce3bec24c\n",
      "Successfully built umap-learn pynndescent\n",
      "Installing collected packages: numpy, llvmlite, numba, pynndescent, umap-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "Successfully installed llvmlite-0.39.1 numba-0.56.4 numpy-1.23.5 pynndescent-0.5.8 umap-learn-0.5.3\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./.conda/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: plotly in ./.conda/lib/python3.10/site-packages (5.14.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.conda/lib/python3.10/site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.10/site-packages (from plotly) (23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install umap-learn\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of my list of sentences: (5, 768)\n"
     ]
    }
   ],
   "source": [
    "my_sentences = [\n",
    "    \"The One Health approach emphasizes the interconnectivity of human, animal, and environmental health.\",\n",
    "    \"The COVID-19 pandemic has underscored the importance of One Health in understanding and preventing zoonotic diseases.\",\n",
    "    \"Collaboration between healthcare professionals, veterinarians, and environmental experts is essential for effective implementation of One Health.\",\n",
    "    \"The workshop on AI and One Health will explore the potential of artificial intelligence to improve health outcomes for humans, animals, and the environment.\",\n",
    "    \"The workshop will take place in Murdoch University\"\n",
    "]\n",
    "\n",
    "sentence_embeddings = model.encode(my_sentences)\n",
    "\n",
    "print(f\"Shape of my list of sentences: {sentence_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/umap/umap_.py:2344: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(my_sentences)\n",
    "\n",
    "umap_emb = umap.UMAP(n_components=2, random_state=42).fit_transform(sentence_embeddings)\n",
    "df['umap-x'] = umap_emb[:, 0]\n",
    "df['umap-y'] = umap_emb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpress\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mscatter(df, x\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mumap-x\u001b[39m\u001b[39m'\u001b[39m, y\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mumap-y\u001b[39m\u001b[39m'\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m fig\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/plotly/basedatatypes.py:3390\u001b[0m, in \u001b[0;36mBaseFigure.show\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3357\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3358\u001b[0m \u001b[39mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[1;32m   3359\u001b[0m \u001b[39mspecified by the renderer argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3386\u001b[0m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3388\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[0;32m-> 3390\u001b[0m \u001b[39mreturn\u001b[39;00m pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/plotly/io/_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(df, x='umap-x', y='umap-y', width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'One Health is an approach calling for the collaborative efforts of multiple a a a a a a a a'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation = pipeline('text-generation', model='bert-base-uncased')\n",
    "text_generation(\"One Health is an approach calling for the collaborative efforts of multiple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
