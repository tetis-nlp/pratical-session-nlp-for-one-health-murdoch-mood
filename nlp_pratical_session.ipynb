{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 style=\"text-align: center;\">NLP for One Health</H1>\n",
    "<h3 style=\"text-align: center;\">From BERT to ChatGPT</h3>\n",
    "\n",
    "The use of artificial intelligence (AI) in One Health has the potential to revolutionize disease detection and outbreak response. Natural Language Processing (NLP) is a subfield of AI that enables computers to understand and analyze large amounts of text data, including social media posts, news articles or medical records. \n",
    "\n",
    "In this practical session, we will explore how NLP can help in the early detection of outbreaks and monitor crisis situations by social mining of newspapers and social media. Specifically, we will start by discussing BERT, a powerful pre-trained language model that can be fine-tuned for specific NLP tasks. We will then move on to ChatGPT, the most popular large language model that can generate human-like responses to natural language inputs. \n",
    "\n",
    "Through the use of these 2 models, we will discuss practical examples of how NLP can be applied in the One Health context, including case studies of outbreak detection and social media monitoring. Participants will have the opportunity to work with BERT and ChatGPT in hands-on exercises to gain practical experience with these powerful tools. By the end of the session, participants will have a deeper understanding of how NLP can be used in the One Health context and be equipped with the skills to apply these techniques in their own work.\n",
    "\n",
    "\n",
    "|   |   |   |   |\n",
    "|---|---|---|---|\n",
    "| <img src=\"https://mood-h2020.eu/wp-content/uploads/2020/10/logo_Mood_texte-dessous_CMJN_vecto-300x136.jpg\" alt=\"mood\"/> | <img src=\"https://www.murdoch.edu.au/ResourcePackages/Murdoch2021/assets/dist/images/logo.svg\" alt=\"murdoch\" /> | <img src=\"https://www.umr-tetis.fr/images/logo-header-tetis.png\" alt=\"tetis\"/> | <img src=\"https://www.inrae.fr/themes/custom/inrae_socle/logo.svg\" alt=\"INRAE\" /> |\n",
    "\n",
    "Speaker: **Rémy DECOUPES** - Research engineer UMR TETIS / INRAE\n",
    "\n",
    "------------------------\n",
    "\n",
    "# 1. BERT\n",
    "\"[Bidirectional Encoder Representations from Transformers - Devlin et al - 2018](https://arxiv.org/abs/1810.04805)\" from Google Research is an open-source pre-trained Language Model. BERT implements the well known \"[Attention is all you need - Vaswani et al - 2017](https://arxiv.org/abs/1706.03762)\"\n",
    "\n",
    "Bert-case was trained on: \n",
    "+ Wikipedia (2.5 Billions of tokens)\n",
    "+ Google books (0.8 Billions of tokens).\n",
    "\n",
    "On two tasks:\n",
    "+ Self-masking\n",
    "+ Next sentence prediction\n",
    "\n",
    "## 1.1 Transformers\n",
    "A python library to easily work with BERT-like models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.conda/lib/python3.10/site-packages (4.27.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.conda/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: torch in ./.conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.conda/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.conda/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: networkx in ./.conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: sympy in ./.conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.conda/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.conda/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.conda/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.conda/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.10/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in ./.conda/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.conda/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.conda/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.1)\n",
      "Requirement already satisfied: wheel in ./.conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.1)\n",
      "Requirement already satisfied: lit in ./.conda/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# installation\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdecoupe/Documents/TETIS/documentation/missions/2023-04_Perth_Murdoch/workshop_MOOD_Murdoch_NLP_for_one_health/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load BERT models\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NLP tasks with BERT\n",
    "Let's use transformers' pipeline on common NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.41287505626678467,\n",
       "  'token': 4111,\n",
       "  'token_str': 'animal',\n",
       "  'sequence': 'the concept of one health recognizes the interconnectedness of human, animal, and environmental health, and emphasizes the need for collaboration across sectors to address complex health challenges.'},\n",
       " {'score': 0.08554541319608688,\n",
       "  'token': 2591,\n",
       "  'token_str': 'social',\n",
       "  'sequence': 'the concept of one health recognizes the interconnectedness of human, social, and environmental health, and emphasizes the need for collaboration across sectors to address complex health challenges.'},\n",
       " {'score': 0.052403345704078674,\n",
       "  'token': 16928,\n",
       "  'token_str': 'occupational',\n",
       "  'sequence': 'the concept of one health recognizes the interconnectedness of human, occupational, and environmental health, and emphasizes the need for collaboration across sectors to address complex health challenges.'},\n",
       " {'score': 0.03880768641829491,\n",
       "  'token': 5177,\n",
       "  'token_str': 'mental',\n",
       "  'sequence': 'the concept of one health recognizes the interconnectedness of human, mental, and environmental health, and emphasizes the need for collaboration across sectors to address complex health challenges.'},\n",
       " {'score': 0.025313813239336014,\n",
       "  'token': 2740,\n",
       "  'token_str': 'health',\n",
       "  'sequence': 'the concept of one health recognizes the interconnectedness of human, health, and environmental health, and emphasizes the need for collaboration across sectors to address complex health challenges.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"The concept of One Health recognizes the interconnectedness of human, [MASK], and environmental health, and emphasizes the need for collaboration across sectors to address complex health challenges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The concept of One Health recognizes the interconnectedness of  of and and and and and and and and'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_sentence_preduction = pipeline('text-generation', model='bert-base-uncased')\n",
    "next_sentence_preduction(\"The concept of One Health recognizes the interconnectedness of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
