{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 style=\"text-align: center;\">NLP for One Health</H1>\n",
    "<h3 style=\"text-align: center;\">From BERT to ChatGPT</h3>\n",
    "\n",
    "|   |   |   |   |\n",
    "|---|---|---|---|\n",
    "| <img src=\"https://mood-h2020.eu/wp-content/uploads/2020/10/logo_Mood_texte-dessous_CMJN_vecto-300x136.jpg\" alt=\"mood\"/> | <img src=\"https://www.murdoch.edu.au/ResourcePackages/Murdoch2021/assets/dist/images/logo.svg\" alt=\"murdoch\" /> | <img src=\"https://www.umr-tetis.fr/images/logo-header-tetis.png\" alt=\"tetis\"/> | <img src=\"https://www.inrae.fr/themes/custom/inrae_socle/logo.svg\" alt=\"INRAE\" /> |\n",
    "\n",
    "Speakers:\n",
    "\n",
    "- **Rémy DECOUPES** - Research engineer UMR TETIS / INRAE\n",
    "- **Maguelonne TEISSEIRE** - Prof. UMR TETIS / INRAE\n",
    "\n",
    "------------------------\n",
    "\n",
    "# Chapter 1. BERT\n",
    "\"[Bidirectional Encoder Representations from Transformers - Devlin et al - 2018](https://arxiv.org/abs/1810.04805)\" from Google Research is an open-source pre-trained Language Model. BERT implements the well known \"[Attention is all you need - Vaswani et al - 2017](https://arxiv.org/abs/1706.03762)\"\n",
    "\n",
    "Bert-case was trained on: \n",
    "+ Wikipedia (2.5 Billions of tokens)\n",
    "+ Google books (0.8 Billions of tokens).\n",
    "\n",
    "On two tasks:\n",
    "+ Self-masking\n",
    "+ Next sentence prediction\n",
    "\n",
    "## 1.1 Introduction: Transformers\n",
    "A python library to easily work with BERT-like models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load BERT models\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NLP tasks with BERT\n",
    "Let's use transformers' pipeline on common NLP tasks\n",
    "### 1.2.1 Self-masking\n",
    "Predict a token masked inside a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"One Health is an approach calling for the collaborative efforts of multiple disciplines working locally, nationally, and globally, to attain optimal health for people, [MASK] and our environment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Next sentence prediction\n",
    "The aim of this NLP task is to tell if the 2nd sentence could be after the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def next_sentence_prediction(sentence_1, sentence_2):\n",
    "  encoding = tokenizer.encode_plus(sentence_1, sentence_2, return_tensors='pt')\n",
    "  logits = nsp_model(**encoding)[0] \n",
    "  probs = softmax(logits, dim=1)\n",
    "  return probs[0][0].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"The One Health Initiative is an interdisciplinary movement to create collaborations between animal, human, and environmental health\"\n",
    "sentence_2 = \"The aim is  to better and more rapidly respond to outbreaks and newly emerging zoonoses and diseases.\"\n",
    "\n",
    "score = next_sentence_prediction(sentence_1, sentence_2)\n",
    "print(score)\n",
    "\n",
    "sentence_3 = \"Murdoch University is located in Perth, Western Australia\"\n",
    "\n",
    "score = next_sentence_prediction(sentence_1, sentence_3)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Name Entities Recognition\n",
    "NER is very usefull to extract specific information. In epidemiology surveillance, we want to extract from new articles the pathogen, the host and the location of an outbreak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', model='dslim/bert-base-NER')\n",
    "ner(\"2 swans found dead in Dordogne\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Diving into the model representations\n",
    "### 1.3.1 Inputs: Natural texts to vectors\n",
    "Inputs / Tokenization.\n",
    "\n",
    "BERT has a vocabulary size of ~30 k Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', return_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"2 swans found dead in Dordogne\"\n",
    "inputs = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "\n",
    "print(f\"Inputs: {inputs} \\n\")\n",
    "print(f\"Inputs_ids: {inputs['input_ids']} \\n\")\n",
    "print(f\"Inputs word ids: {inputs.word_ids()} \\n\")\n",
    "print(f\"Inputs to words: {tokenizer.tokenize(text)} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on two points:\n",
    "\n",
    "+ **Sub-tokenization**: as BERT knows only ~30K Tokens, tokens unknown have to be splitted into subtoken such as **Dordogne** becomes [do, ##rdo, ##gne].\n",
    "+ **Special token**: Two tokens have been added: 101 and 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Token ID = 2079, 20683, 10177: {tokenizer.convert_ids_to_tokens([2079, 20683, 10177])} \\\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Shape of inputs: {inputs['input_ids'].shape} | Number of words: {len(tokenizer.tokenize(text))} \\\\n\")\n",
    "print(f\"Token ID = 101: {tokenizer.convert_ids_to_tokens(101)} \\\\n\")\n",
    "print(f\"Token ID = 102: {tokenizer.convert_ids_to_tokens(102)} \\\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.2 Embedding: \n",
    "Vector representation of semantic informations of texts.\n",
    "\n",
    "Each known token has a vector representation from the pre-trained BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_matrix = model.embeddings.word_embeddings.weight\n",
    "\n",
    "print(f\"BERT embedding matrix: {embedding_matrix} \\n\\n Matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Embeddings of Swan (id=26699): {embedding_matrix[26699]}\\n\\n\")\n",
    "print(f\"Embeddings of Dordogne (id=2079, 20683, 10177): {embedding_matrix[[2079, 20683, 10177]]} \\\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the strength of BERT is to capture the **contextualized** semantics of a word in its sentence. To explore this embeddings, let's see the last layer representation **last_hidden_states**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import math \n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"2 swans found dead in Dordogne\"\n",
    "input_ids = tokenizer.encode(input_text)\n",
    "input_ids = torch.tensor([input_ids])\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]\n",
    "last_hidden_states_mean = last_hidden_states.mean(1)\n",
    "\n",
    "print(f\"Embedding shape: {last_hidden_states.shape}\\n\\n\")\n",
    "print(f\"Embeddings of Swan inside its sentence (word_id=2): {last_hidden_states[0, 2, :]}\\n\\n\")\n",
    "euclidean_distance = math.sqrt(sum((embedding_matrix[26699] - last_hidden_states[0, 2, :])**2))\n",
    "print(f\"Euclidean distance between Sawn (in pre-trained) and Swan in our sentence: {euclidean_distance}\")\n",
    "euclidean_distance = math.sqrt(sum((embedding_matrix[2757] - last_hidden_states[0, 4, :])**2))\n",
    "print(f\"Euclidean distance between dead (in pre-trained) and dead in our sentence: {euclidean_distance}\")\n",
    "euclidean_distance = math.sqrt(sum((last_hidden_states[0, 4, :] - last_hidden_states[0, 2, :])**2))\n",
    "print(f\"Euclidean distance between dead and Swan in our sentence: {euclidean_distance}\")\n",
    "euclidean_distance = math.sqrt(sum((embedding_matrix[2757] - embedding_matrix[26699])**2))\n",
    "print(f\"Euclidean distance between dead and Swan in pre-trained: {euclidean_distance}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Visualize sentences embedding\n",
    "The meaning of a sentence is included in this embeddings of its tokens. We propose here too visualize it:\n",
    "\n",
    "+ Retrieve embedding of a sentence (and not at a token-level) using [sentence embedding](https://github.com/UKPLab/sentence-transformers). We are going to use **all_mpnet-base-v2** first-ranked model by [Sentence BERT benchmark](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models).\n",
    "+ Reduce dimension to plot embeddings on a 2-D figure using [UMAP](https://umap-learn.readthedocs.io/en/latest/)\n",
    "+ Visualize it interactively with [plotly.express](https://plotly.com/python/plotly-express/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install umap-learn\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "my_sentences_dict = [\n",
    "    {\"sentence\": \"One Health is an interdisciplinary approach that recognizes the interconnectedness of human, animal, and environmental health.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"The One Health concept emphasizes the need for collaboration across sectors to address complex health challenges.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"A One Health approach is essential for understanding and preventing emerging infectious diseases.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"One Health research aims to improve the health and well-being of both people and animals.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"One Health initiatives promote a holistic and integrated approach to health and disease.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"One Health is a global initiative that aims to improve public health and environmental sustainability.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"The One Health framework emphasizes the importance of collaboration, communication, and coordination across different fields.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"One Health research has contributed to our understanding of zoonotic diseases such as Ebola and COVID-19.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"One Health approaches can help us address complex health challenges such as antimicrobial resistance.\", \"category\": \"one-health\"},\n",
    "    {\"sentence\": \"One Health recognizes the interconnectedness of human, animal, and environmental health and aims to promote the health and well-being of all three.\", \"category\": \"one-health\"},\n",
    "\n",
    "    {\"sentence\": \"The workshop on AI and One Health brought together experts from diverse fields to explore new opportunities for collaboration.\", \"category\": \"workship-ai-onehealth\"},\n",
    "    {\"sentence\": \"The AI and One Health workshop focused on using advanced technologies to improve public health outcomes.\", \"category\": \"workship-ai-onehealth\"},\n",
    "    {\"sentence\": \"The workshop on AI and One Health provided a platform for discussing the ethical and social implications of using AI in healthcare.\", \"category\": \"workship-ai-onehealth\"},\n",
    "    {\"sentence\": \"The AI and One Health workshop explored new ways to use data and analytics to improve health outcomes for both people and animals.\", \"category\": \"workship-ai-onehealth\"},\n",
    "    {\"sentence\": \"The workshop on AI and One Health highlighted the importance of interdisciplinary collaboration in addressing complex health challenges.\", \"category\": \"workship-ai-onehealth\"},\n",
    "    \n",
    "    {\"sentence\": \"Murdoch University is a public research university located in Perth, Western Australia.\", \"category\": \"murdoch-uni\"},\n",
    "    {\"sentence\": \"The campus of Murdoch University is situated on a large nature reserve, providing a unique and tranquil environment for learning and research.\", \"category\": \"murdoch-uni\"},\n",
    "    {\"sentence\": \"Murdoch University is known for its strong programs in veterinary science and animal health.\", \"category\": \"murdoch-uni\"},\n",
    "    {\"sentence\": \"The School of Health Sciences at Murdoch University offers a range of programs in nursing, psychology, and public health.\", \"category\": \"murdoch-uni\"},\n",
    "    {\"sentence\": \"Murdoch University has a strong commitment to sustainability and environmental stewardship, reflected in its research and operations.\", \"category\": \"murdoch-uni\"}\n",
    "]\n",
    "df = pd.DataFrame(my_sentences_dict)\n",
    "print(df.head(2))\n",
    "\n",
    "sentence_embeddings = model.encode(df[\"sentence\"].values)\n",
    "\n",
    "print(f\"Shape of my list of sentences: {sentence_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_emb = umap.UMAP(n_components=2, random_state=42).fit_transform(sentence_embeddings)\n",
    "df['umap-x'] = umap_emb[:, 0]\n",
    "df['umap-y'] = umap_emb[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(df, x='umap-x', y='umap-y', custom_data=[\"sentence\"], color=\"category\", width=800, height=800)\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"ColX: %{x}\",\n",
    "        \"ColY: %{y}\",\n",
    "        \"Col1: %{customdata[0]}\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Fine-tuning\n",
    "**Use case**: Epidemiological surveillance (Event-based) for AMR (Anti Microbial Resistance)\n",
    "\n",
    "The aims of this section is to train, from a pre-trained BERT model, a text classification of news (press articles):\n",
    "\n",
    "+ \"New Information\": The new is dealing with an outbreak\n",
    "+ \"General Information\": The press article is in the theme (it talks about AMR) but does not mention any new emergence\n",
    "+ \"Not relevant\": the press article is off-topic\n",
    "\n",
    "The data sources are:\n",
    "\n",
    "+ [ProMED](https://promedmail.org/); From the International Society of Infectious Diseases (ISID) since 1994, expert moderators provide written commentary (a context) to the press articles reported.\n",
    "+ [HealthMap](https://www.healthmap.org): Is an automated and curated aggregator of a broad range of data sources (Twitter, Google News, Baidu and ProMED)\n",
    "+ [MedISys](https://medisys.newsbrief.eu/medisys/homeedition/en/home.html): A web-based information monitoring system developped by the European Comission \n",
    "+ [PADI-web](https://padi-web.cirad.fr/); Partly developed by researchers from UMR TETIS and used by the French epidemix intellignece team in animal health. It's an autmated tool that monitors the Google News aggregator in sixteen languages.\n",
    "\n",
    "The data is accessible at the following link: [MOOD - News AMR dataset - Hackathon 2022](https://entrepot.recherche.data.gouv.fr/dataset.xhtml?persistentId=doi:10.57745/MPNSPH). It comes from the [MOOD](https://mood-h2020.eu/time-for-a-mood-hack-antimicrobial-resistance-hackathon/) project.\n",
    "\n",
    "\n",
    "![MOOD](https://mood-h2020.eu/wp-content/uploads/2020/10/logo_Mood_cmjn_black-1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Download the data and upload it to your notebook\n",
    "\n",
    "1. Download the data (giving your personnal information is not mandatory, you can supply click on the accept button): [from the following link](https://entrepot.recherche.data.gouv.fr/dataset.xhtml?persistentId=doi:10.57745/MPNSPH)\n",
    "2. Extract the 4 files beginning with `D1_`\n",
    "3. Upload them to colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as p\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df_tmp = pd.DataFrame()\n",
    "\n",
    "for filename in uploaded:\n",
    "  df_tmp = pd.read_csv(io.StringIO(uploaded[filename].decode(\"utf-8\")), sep = \",\")\n",
    "  df = pd.concat([df, df_tmp], ignore_index=True)\n",
    "\n",
    "# filter out \"don't know\"\n",
    "df = df[df[\"Classification1\"] != \"Don't know\"]\n",
    "\n",
    "# filter out Null values\n",
    "df = df[df[\"Title\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import ClassLabel\n",
    "\n",
    "df[\"label_name\"] = df[\"Classification1\"]\n",
    "df[\"label\"] = pd.Categorical(df[\"label_name\"], ordered=True).codes\n",
    "\n",
    "mapLabels = pd.DataFrame(df.groupby(['label_name', 'label']).count())\n",
    "#drop count column\n",
    "# mapLabels.drop(['news'], axis = 1, inplace = True)\n",
    "label2Index = mapLabels.to_dict(orient='index')\n",
    "index2label = {}\n",
    "for key in label2Index:\n",
    "  print (f\"{key[1]} -> {key[0]}\")\n",
    "  index2label[key[1]] = key[0]\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df[[\"Title\", \"label\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Prepare the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pretrained_model = 'bert-base-uncased'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "import numpy as np\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    try:\n",
    "        tokenized_batch = tokenizer(batch['Title'], padding=True, truncation=True, max_length=128)\n",
    "        return tokenized_batch\n",
    "    except:\n",
    "        print(f\"error with Title: {batch['Title']}\")\n",
    "        print(f\"error with Title len: {len(batch['Title'])}\")\n",
    "                     \n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# mélanger les jeux de données:\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, num_labels=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    print(metric.compute(predictions=predictions, references=labels))\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  load_best_model_at_end=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Text generation with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generation = pipeline('text-generation', model='bert-base-uncased')\n",
    "text_generation(\"One Health is an approach calling for the collaborative efforts of multiple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = pipeline(\"text2text-generation\", model='bert-base-uncased')\n",
    "qa(\"What does 'one health concept' mean ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
